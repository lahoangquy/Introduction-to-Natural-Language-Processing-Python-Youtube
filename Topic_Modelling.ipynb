{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modelling",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2EwGO4VtDCb",
        "outputId": "1fb52418-afc3-46e1-8505-40f496b0f162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN2uBAOWtZyi",
        "outputId": "41b27cd3-b3a1-49a9-d6ac-c22b1889b117"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic modeling is the process of identifying patterns in text data that correspond to a topic. If the text contains multiple topics, then this technique can be used to identify and separate those themse within the input text. This techque can be used to unvoer hidden thematic stricture in a given set of documents. \n",
        "\n",
        "Tolic modeling helps us to organize documents in an optimal way, which can then be used for analysis. One to thing to remember that about topic modeling algorithms is that they do not need labeled data. it is like unsuperviesed learning in that it will identify the patterns on its own. Given the huge amount of text data generated on the internet, topic modlling is important because it enables the summarization of vast amount of data which would otherwise not be possible.\n",
        "\n",
        "Latent Dirictlet Allocation is a topic modeling technique, the underlying concept of which is that given a pice of text is a combination of multiple topics.\n",
        "The library that will be used is gensim"
      ],
      "metadata": {
        "id": "HxXwWpiuwL62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR7jJ99IxUxy",
        "outputId": "ebfc56eb-3379-445f-bde0-7f886f6a54a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from gensim import models, corpora"
      ],
      "metadata": {
        "id": "ST1fijthxZ4v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the input data. This input data contains 10 line-separated sentences\n",
        "def load_data(input_file):\n",
        "  data= []\n",
        "  with open(input_file, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      data.append(line[:-1])\n",
        "  return data"
      ],
      "metadata": {
        "id": "VwA1rpbHxqie"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to process the input data\n",
        "def process(input_text):\n",
        "    # Create a regular expression tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "    # Create a Snowball stemmer \n",
        "    stemmer = SnowballStemmer('english')\n",
        "\n",
        "    # Get the list of stop words \n",
        "    stop_words = stopwords.words('english')\n",
        "    \n",
        "    # Tokenize the input string\n",
        "    tokens = tokenizer.tokenize(input_text.lower())\n",
        "\n",
        "    # Remove the stop words \n",
        "    tokens = [x for x in tokens if not x in stop_words]\n",
        "    \n",
        "    # Perform stemming on the tokenized words \n",
        "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
        "\n",
        "    return tokens_stemmed"
      ],
      "metadata": {
        "id": "goPs9PZIyCyV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load input data\n",
        "data = load_data('data.txt')"
      ],
      "metadata": {
        "id": "L_o4m_LRzq9u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list for sentence tokens\n",
        "tokens = [process(x) for x in data]"
      ],
      "metadata": {
        "id": "mTW25SI9zy_O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary based on the sentence tokens\n",
        "dict_tokens = corpora.Dictionary(tokens)"
      ],
      "metadata": {
        "id": "b_RzdAdIz5Ja"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a document-term matrix using the sentence tokens\n",
        "doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]"
      ],
      "metadata": {
        "id": "3hgnHn2E0Akm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is we need to provide the number of topics as the input parameter. In this project, the input text has 2 distinct groups"
      ],
      "metadata": {
        "id": "zgd8gUZ20P5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of topics for the LDA model\n",
        "num_topics=2"
      ],
      "metadata": {
        "id": "fZTwhyY80O00"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the Latent Dirictlet Allocation\n",
        "ldamodel = models.ldamodel.LdaModel(doc_term_mat, num_topics=num_topics, id2word=dict_tokens, passes=25)"
      ],
      "metadata": {
        "id": "PrhPNu980d_f"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 5 contributing words for each topic\n",
        "num_words = 5\n",
        "print('\\nTop ' + str(num_words) + ' contributing words to each topic:')\n",
        "for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
        "    print('\\nTopic', item[0])\n",
        "\n",
        "    # Print the contributing words along with their \n",
        "    #relative contributions \n",
        "    list_of_strings = item[1].split(' + ')\n",
        "    for text in list_of_strings:\n",
        "        weight = text.split('*')[0]\n",
        "        word = text.split('*')[1]\n",
        "        print(word, '==>', str(round(float(weight) * 100, 2)) + '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TldD6kye0vdZ",
        "outputId": "b3fd07c0-e8a6-46f8-b9a9-b8635329bd8e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 contributing words to each topic:\n",
            "\n",
            "Topic 0\n",
            "\"mathemat\" ==> 4.4%\n",
            "\"set\" ==> 3.1%\n",
            "\"structur\" ==> 3.1%\n",
            "\"call\" ==> 3.1%\n",
            "\"algebra\" ==> 1.9%\n",
            "\n",
            "Topic 1\n",
            "\"empir\" ==> 3.9%\n",
            "\"cultur\" ==> 2.8%\n",
            "\"europ\" ==> 2.8%\n",
            "\"time\" ==> 2.8%\n",
            "\"peopl\" ==> 2.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above result, it does a reasonably good job of seperating 2 topics- mathematics and history. If we look at the text we can verify that each sentence is either about mathematics or history."
      ],
      "metadata": {
        "id": "RhUCr17-1zOy"
      }
    }
  ]
}